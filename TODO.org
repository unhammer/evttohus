* TODO try spelling lms instead of forms
  : git checkout spell-lms-instead-of-forms
* TODO try grabbing word all lists from langs/smj etc. and adding to speller material
* TODO add "word-lists" to corpus before make-freq?

* TODO also try non-fad words for the sources that give the best candidates
  or for high frequency candidates etc.

* TODO simple rules for loans
  | nob          | smj            | sme           |
  |--------------+----------------+---------------|
  | transkribere | transkribierit | transkriberit |
  | initiere     | initieret      | initieret     |

  If we need nob, we can't rely on xfst alone.

* TODO kintel, for nobsmj
* TODO pretty.sh should include a "source" field or something
  when we get kintel in there, we need to differentiate if something
  came from nobsmj or smesmj

* TODO shorten corpora to size of the smallest for comparable frequencies?
  Currently, =pretty.sh= will simply divide the sum of the larger by
  the sum of the smallest corpus.

* DONE use forms or lms for frequency?
  Some of the dictionary entries are have 0 hits in lms, but hits in
  forms (TV-nyheter), but we still get less 0's by using lms

  Could lemmatise before looking up frequency, but this is complicated
  enough already …

  → We just sum them. This will give dictionary-forms like N.Sg.Nom
  artificially high frequencies, but *we want to prioritise such hits
  anyway*.

* TODO kwic-annotate out-files

* TODO run through usmjNorm and take lemmas for words marked Err/Sub ?
  
* TODO lemmatising para/toktmx: analyse forms first, then add to sentences
  No idea how to make /usr/local/bin/lookup ignore any input, so no
  good way to split sentences and then patch them back together.

  Simple solution: just ana all the words, read it into an awk assoc
  table and apply ana's with awk.
